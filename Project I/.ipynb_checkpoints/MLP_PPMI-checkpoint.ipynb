{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "#from xgboost import predict_proba\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV #网格搜索\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split                 \n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.utils.data as Data\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'NC', 'PD'])\n",
      "(169, 294)\n",
      "(374, 294)\n"
     ]
    }
   ],
   "source": [
    "m = loadmat(\"PPMI.mat\") \n",
    "\n",
    "m.keys()\n",
    "print(m.keys())\n",
    "print(m['NC'].shape)\n",
    "print(m['PD'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD = m['PD']\n",
    "NC = m['NC']\n",
    "\n",
    "PD = pd.DataFrame(PD)\n",
    "NC = pd.DataFrame(NC)\n",
    "\n",
    "PD['label'] = [0]*374\n",
    "NC['label'] = [1]*169\n",
    "\n",
    "j = 10\n",
    "pca = PCA(n_components=j)#PCA降维\n",
    "\n",
    "df = PD.append(NC)\n",
    "label = df['label']\n",
    "df=df.drop(['label'],axis = 1)\n",
    "#df =  pca.fit_transform(df)\n",
    "\n",
    "PD=PD.drop(['label'],axis = 1)\n",
    "NC=NC.drop(['label'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm_scaler = preprocessing.MinMaxScaler()\n",
    "#df = mm_scaler.fit_transform(df)\n",
    "df = pd.DataFrame(df)\n",
    "df = df*1.0\n",
    "x_train,x_test,y_train,y_test = train_test_split(df , label, test_size = 0.1 )  \n",
    "train_dataset = Data.TensorDataset(torch.tensor(x_train.values), torch.tensor(y_train.values))\n",
    "test_dataset = Data.TensorDataset(torch.tensor(x_test.values), torch.tensor(y_test.values))\n",
    "batch_size = 1000\n",
    "num_workers = 0\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(488, 294)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear1): Linear(in_features=294, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (batchNorm1d1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear2): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (batchNorm1d2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_inputs = 294\n",
    "num_outputs = 2\n",
    "hidden_dim1=512\n",
    "hidden_dim2=16\n",
    "\n",
    "#定义模型（添加）\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(LinearNet,self).__init__()\n",
    "        #self.flatten = d2l.FlattenLayer()\n",
    "        self.linear1 = nn.Linear(num_inputs,hidden_dim1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.batchNorm1d1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.batchNorm1d2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.linear3 = nn.Linear(hidden_dim2, num_outputs)\n",
    "        self.dropout = nn.Dropout(p=0.05)  # dropout训练\n",
    "    def forward(self, x): # x shape: (batch, 1, 28, 28)\n",
    "        y = self.linear1(x.view(x.shape[0],-1))\n",
    "        y = self.batchNorm1d1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        y = self.batchNorm1d1(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        #y = self.batchNorm1d2(y)\n",
    "        y = self.linear3(y)\n",
    "        #y = self.batchNorm1d2(y)\n",
    "        #y = self.relu(y)\n",
    "        return y\n",
    "    \n",
    "net = LinearNet(num_inputs,num_outputs)\n",
    "#for params in net.parameters(): \n",
    "#    init.normal_(params, mean=0, std=0.01)   #参数初始化\n",
    "print(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET(\n",
      "  (net): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=294, out_features=512, bias=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Linear(in_features=128, out_features=16, bias=True)\n",
      "    (8): Dropout(p=0.05, inplace=False)\n",
      "    (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Linear(in_features=16, out_features=2, bias=True)\n",
      "    (11): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NET, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(294, 512),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "net = NET()\n",
    "#for params in net.parameters(): \n",
    "#    init.normal_(params, mean=0, std=0.01)   #参数初始化\n",
    "print(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                pred = net(X.to(device).to(torch.float32)).argmax(dim=1)\n",
    "                net.eval() # 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device).to(torch.float32)).argmax(dim=1) == y.to(device).to(torch.float32)).float().sum().cpu().item()\n",
    "                net.train() # 改回训练模式\n",
    "            else: # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n , pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_results(pred,y):\n",
    "    from sklearn.metrics import accuracy_score,roc_auc_score,auc,confusion_matrix,roc_curve\n",
    "    fpr,tpr,threshold = roc_curve(y, pred) \n",
    "    roc_auc= auc(fpr,tpr)\n",
    "\n",
    "    if(len(set(y))!=1):\n",
    "        roc=roc_auc_score(y, pred)\n",
    "        sensitivity=confusion_matrix(y, pred)[0][0]/(confusion_matrix(y, pred)[0][0]+confusion_matrix(y, pred)[0][1])\n",
    "        specificity=confusion_matrix(y, pred)[1][1]/(confusion_matrix(y, pred)[1][1]+confusion_matrix(y, pred)[1][0])\n",
    "    else:\n",
    "        roc=0\n",
    "        sensitivity=0\n",
    "        specificity=0\n",
    "    return roc,sensitivity,specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear1): Linear(in_features=294, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (batchNorm1d1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear2): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (batchNorm1d2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear3): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      ")\n",
      "0.5 1.0 0.0\n",
      "epoch 1, loss 0.6434, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.03743791580200195\n",
      "0.5107142857142857 0.9714285714285714 0.05\n",
      "epoch 2, loss 0.5953, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.031914710998535156\n",
      "0.5 1.0 0.0\n",
      "epoch 3, loss 0.5656, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.025931119918823242\n",
      "0.5 1.0 0.0\n",
      "epoch 4, loss 0.5323, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02991938591003418\n",
      "0.4857142857142857 0.9714285714285714 0.0\n",
      "epoch 5, loss 0.5153, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.026927709579467773\n",
      "0.4857142857142857 0.9714285714285714 0.0\n",
      "epoch 6, loss 0.4989, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030918121337890625\n",
      "0.5107142857142857 0.9714285714285714 0.05\n",
      "epoch 7, loss 0.4762, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390932083129883\n",
      "0.49642857142857144 0.9428571428571428 0.05\n",
      "epoch 8, loss 0.4565, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.031914472579956055\n",
      "0.5107142857142857 0.9714285714285714 0.05\n",
      "epoch 9, loss 0.4396, train acc 0.695, test acc 0.364\n",
      "该轮训练用时： 0.028922319412231445\n",
      "0.4714285714285714 0.9428571428571428 0.0\n",
      "epoch 10, loss 0.4211, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.031915903091430664\n",
      "0.49642857142857144 0.9428571428571428 0.05\n",
      "epoch 11, loss 0.4077, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.030917644500732422\n",
      "0.49642857142857144 0.9428571428571428 0.05\n",
      "epoch 12, loss 0.3962, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.03191351890563965\n",
      "0.49642857142857144 0.9428571428571428 0.05\n",
      "epoch 13, loss 0.3802, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.03291177749633789\n",
      "0.49642857142857144 0.9428571428571428 0.05\n",
      "epoch 14, loss 0.3612, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03391003608703613\n",
      "0.45714285714285713 0.9142857142857143 0.0\n",
      "epoch 15, loss 0.3519, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02992534637451172\n",
      "0.49642857142857144 0.9428571428571428 0.05\n",
      "epoch 16, loss 0.3364, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03190922737121582\n",
      "0.4535714285714286 0.8571428571428571 0.05\n",
      "epoch 17, loss 0.3190, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.03690075874328613\n",
      "0.4928571428571429 0.8857142857142857 0.1\n",
      "epoch 18, loss 0.3078, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.034906864166259766\n",
      "0.4535714285714286 0.8571428571428571 0.05\n",
      "epoch 19, loss 0.2967, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.03191423416137695\n",
      "0.4535714285714286 0.8571428571428571 0.05\n",
      "epoch 20, loss 0.2788, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.03590512275695801\n",
      "0.4535714285714286 0.8571428571428571 0.05\n",
      "epoch 21, loss 0.2682, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.034906625747680664\n",
      "0.4785714285714286 0.8571428571428571 0.1\n",
      "epoch 22, loss 0.2554, train acc 0.305, test acc 0.364\n",
      "该轮训练用时： 0.03291130065917969\n",
      "0.5035714285714286 0.8571428571428571 0.15\n",
      "epoch 23, loss 0.2397, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03291201591491699\n",
      "0.4928571428571429 0.8857142857142857 0.1\n",
      "epoch 24, loss 0.2292, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.032912492752075195\n",
      "0.4785714285714286 0.8571428571428571 0.1\n",
      "epoch 25, loss 0.2171, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.028921842575073242\n",
      "0.4785714285714286 0.8571428571428571 0.1\n",
      "epoch 26, loss 0.2102, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.028922557830810547\n",
      "0.4392857142857143 0.8285714285714286 0.05\n",
      "epoch 27, loss 0.1918, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.0331723690032959\n",
      "0.4392857142857143 0.8285714285714286 0.05\n",
      "epoch 28, loss 0.1811, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03167009353637695\n",
      "0.4785714285714286 0.8571428571428571 0.1\n",
      "epoch 29, loss 0.1690, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.032911062240600586\n",
      "0.4785714285714286 0.8571428571428571 0.1\n",
      "epoch 30, loss 0.1567, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.029920101165771484\n",
      "0.4785714285714286 0.8571428571428571 0.1\n",
      "epoch 31, loss 0.1490, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.031914472579956055\n",
      "0.4642857142857143 0.8285714285714286 0.1\n",
      "epoch 32, loss 0.1408, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.032912492752075195\n",
      "0.4928571428571429 0.8857142857142857 0.1\n",
      "epoch 33, loss 0.1323, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03490591049194336\n",
      "0.5035714285714286 0.8571428571428571 0.15\n",
      "epoch 34, loss 0.1254, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030917644500732422\n",
      "0.5035714285714286 0.8571428571428571 0.15\n",
      "epoch 35, loss 0.1175, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.026928186416625977\n",
      "0.4785714285714286 0.8571428571428571 0.1\n",
      "epoch 36, loss 0.1072, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.027925491333007812\n",
      "0.4892857142857142 0.8285714285714286 0.15\n",
      "epoch 37, loss 0.1019, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03291153907775879\n",
      "0.5142857142857142 0.8285714285714286 0.2\n",
      "epoch 38, loss 0.0943, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.032912254333496094\n",
      "0.475 0.8 0.15\n",
      "epoch 39, loss 0.0874, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.036902427673339844\n",
      "0.4642857142857143 0.8285714285714286 0.1\n",
      "epoch 40, loss 0.0814, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03989219665527344\n",
      "0.5285714285714286 0.8571428571428571 0.2\n",
      "epoch 41, loss 0.0770, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03690147399902344\n",
      "0.4892857142857142 0.8285714285714286 0.15\n",
      "epoch 42, loss 0.0684, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03091740608215332\n",
      "0.5 0.8 0.2\n",
      "epoch 43, loss 0.0613, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030916690826416016\n",
      "0.45000000000000007 0.8 0.1\n",
      "epoch 44, loss 0.0602, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.029920339584350586\n",
      "0.5142857142857142 0.8285714285714286 0.2\n",
      "epoch 45, loss 0.0541, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02991962432861328\n",
      "0.4607142857142857 0.7714285714285715 0.15\n",
      "epoch 46, loss 0.0515, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.029920339584350586\n",
      "0.5 0.8 0.2\n",
      "epoch 47, loss 0.0494, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03277277946472168\n",
      "0.4892857142857142 0.8285714285714286 0.15\n",
      "epoch 48, loss 0.0457, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390979766845703\n",
      "0.475 0.8 0.15\n",
      "epoch 49, loss 0.0404, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03291130065917969\n",
      "0.5 0.8 0.2\n",
      "epoch 50, loss 0.0446, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.034906625747680664\n",
      "0.475 0.8 0.15\n",
      "epoch 51, loss 0.0357, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03191542625427246\n",
      "0.475 0.8 0.15\n",
      "epoch 52, loss 0.0319, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030916929244995117\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 53, loss 0.0309, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.0329127311706543\n",
      "0.4892857142857142 0.8285714285714286 0.15\n",
      "epoch 54, loss 0.0297, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.028922557830810547\n",
      "0.4607142857142857 0.7714285714285715 0.15\n",
      "epoch 55, loss 0.0262, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390812873840332\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 56, loss 0.0230, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03391075134277344\n",
      "0.525 0.8 0.25\n",
      "epoch 57, loss 0.0235, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.032910823822021484\n",
      "0.5357142857142857 0.7714285714285715 0.3\n",
      "epoch 58, loss 0.0210, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390932083129883\n",
      "0.5142857142857142 0.8285714285714286 0.2\n",
      "epoch 59, loss 0.0206, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390979766845703\n",
      "0.4607142857142857 0.7714285714285715 0.15\n",
      "epoch 60, loss 0.0168, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390908241271973\n",
      "0.525 0.8 0.25\n",
      "epoch 61, loss 0.0169, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03191494941711426\n",
      "0.49642857142857144 0.7428571428571429 0.25\n",
      "epoch 62, loss 0.0147, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02792525291442871\n",
      "0.5107142857142857 0.7714285714285715 0.25\n",
      "epoch 63, loss 0.0154, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030916452407836914\n",
      "0.475 0.8 0.15\n",
      "epoch 64, loss 0.0179, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030917882919311523\n",
      "0.4607142857142857 0.7714285714285715 0.15\n",
      "epoch 65, loss 0.0120, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.031914710998535156\n",
      "0.4607142857142857 0.7714285714285715 0.15\n",
      "epoch 66, loss 0.0128, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030916929244995117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5142857142857142 0.8285714285714286 0.2\n",
      "epoch 67, loss 0.0138, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030917882919311523\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 68, loss 0.0155, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02892279624938965\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 69, loss 0.0126, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.028922080993652344\n",
      "0.48214285714285715 0.7142857142857143 0.25\n",
      "epoch 70, loss 0.0119, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02892327308654785\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 71, loss 0.0111, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030918121337890625\n",
      "0.5 0.8 0.2\n",
      "epoch 72, loss 0.0108, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390908241271973\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 73, loss 0.0133, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03291153907775879\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 74, loss 0.0130, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.034906625747680664\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 75, loss 0.0090, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03789925575256348\n",
      "0.5142857142857142 0.8285714285714286 0.2\n",
      "epoch 76, loss 0.0083, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03091716766357422\n",
      "0.525 0.8 0.25\n",
      "epoch 77, loss 0.0082, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.025929927825927734\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 78, loss 0.0078, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03291153907775879\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 79, loss 0.0072, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.033910274505615234\n",
      "0.525 0.8 0.25\n",
      "epoch 80, loss 0.0072, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.030916690826416016\n",
      "0.4607142857142857 0.7714285714285715 0.15\n",
      "epoch 81, loss 0.0082, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02892279624938965\n",
      "0.4857142857142857 0.7714285714285715 0.2\n",
      "epoch 82, loss 0.0074, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390932083129883\n",
      "0.525 0.8 0.25\n",
      "epoch 83, loss 0.0078, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03789830207824707\n",
      "0.5107142857142857 0.7714285714285715 0.25\n",
      "epoch 84, loss 0.0083, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.0399169921875\n",
      "0.525 0.8 0.25\n",
      "epoch 85, loss 0.0067, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03490567207336426\n",
      "0.5 0.8 0.2\n",
      "epoch 86, loss 0.0057, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03191423416137695\n",
      "0.5214285714285715 0.7428571428571429 0.3\n",
      "epoch 87, loss 0.0057, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03291153907775879\n",
      "0.48214285714285715 0.7142857142857143 0.25\n",
      "epoch 88, loss 0.0052, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03091740608215332\n",
      "0.4714285714285714 0.7428571428571429 0.2\n",
      "epoch 89, loss 0.0068, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.029920101165771484\n",
      "0.45714285714285713 0.7142857142857143 0.2\n",
      "epoch 90, loss 0.0049, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02892279624938965\n",
      "0.4714285714285714 0.7428571428571429 0.2\n",
      "epoch 91, loss 0.0064, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03191566467285156\n",
      "0.49642857142857144 0.7428571428571429 0.25\n",
      "epoch 92, loss 0.0053, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.034905433654785156\n",
      "0.48214285714285715 0.7142857142857143 0.25\n",
      "epoch 93, loss 0.0066, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.034906864166259766\n",
      "0.5214285714285715 0.7428571428571429 0.3\n",
      "epoch 94, loss 0.0046, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03690195083618164\n",
      "0.4714285714285714 0.7428571428571429 0.2\n",
      "epoch 95, loss 0.0048, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.036901235580444336\n",
      "0.4714285714285714 0.7428571428571429 0.2\n",
      "epoch 96, loss 0.0054, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.031914472579956055\n",
      "0.49642857142857144 0.7428571428571429 0.25\n",
      "epoch 97, loss 0.0042, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03390955924987793\n",
      "0.5107142857142857 0.7714285714285715 0.25\n",
      "epoch 98, loss 0.0052, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.03091716766357422\n",
      "0.5107142857142857 0.7714285714285715 0.25\n",
      "epoch 99, loss 0.0045, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.02892327308654785\n",
      "0.5107142857142857 0.7714285714285715 0.25\n",
      "epoch 100, loss 0.0047, train acc 0.695, test acc 0.636\n",
      "该轮训练用时： 0.029918909072875977\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "now_time = time.time()\n",
    "losses = []\n",
    "accuracies = []\n",
    "net = NET()\n",
    "plot_test = []\n",
    "plot_train = []\n",
    "#损失函数和优化器\n",
    "net = NET()\n",
    "#for params in net.parameters(): \n",
    "#    init.normal_(params, mean=0, std=0.01)   #参数初始化\n",
    "print(net) \n",
    "\n",
    "for X, y in test_iter:\n",
    "    y_true = y\n",
    "#roc,sensitivity,specificity = []\n",
    "\n",
    "    \n",
    "num_epochs = 100\n",
    "now_time = time.time()\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "plot_test = []\n",
    "plot_train = []\n",
    "plot_roc = []\n",
    "plot_sensitivity = []\n",
    "plot_specificity = []\n",
    "\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(\n",
    "                            net.parameters(), \n",
    "                            lr = LR,\n",
    "                            weight_decay = 0.0001\n",
    "                             )\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        for p in optimizer.param_groups:\n",
    "            p['lr'] *= 0.95\n",
    "    \n",
    "    train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X.to(torch.float32))\n",
    "        l = loss(y_hat,y).sum()#数据集损失函数的值=每个样本的损失函数值的和。            \n",
    "        optimizer.zero_grad()#对优化函数梯度清零\n",
    "        l.backward()#对损失函数求梯度\n",
    "        optimizer.step() \n",
    "        \n",
    "        running_loss += l.item()\n",
    "        n += y.shape[0]\n",
    "    #print(train_acc_sum/n)\n",
    "    train_acc , train_pred=evaluate_accuracy(train_iter, net)\n",
    "    test_acc , test_pred= evaluate_accuracy(test_iter, net)\n",
    "    plot_test.append(test_acc)\n",
    "    \n",
    "    a,b,c = all_results(test_pred,y_true)\n",
    "    plot_roc .append(a)\n",
    "    plot_sensitivity .append(b)\n",
    "    plot_specificity .append(c)\n",
    "    \n",
    "    plot_train.append(train_acc)\n",
    "    accuracies.append(test_acc)\n",
    "    losses.append(running_loss)\n",
    "    print(a,b,c)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "          % (epoch + 1, running_loss, train_acc , test_acc))\n",
    "    pre_time = now_time\n",
    "    now_time = time.time()\n",
    "    print('该轮训练用时：',now_time-pre_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 次交叉验证\n",
      "第 1 折,train acc 0.809, test acc 0.727,test roc 0.683,test sensitivity 0.972,test specificity 0.421\n",
      "第 2 折,train acc 0.824, test acc 0.727,test roc 0.578,test sensitivity 0.921,test specificity 0.235\n",
      "第 3 折,train acc 0.824, test acc 0.673,test roc 0.568,test sensitivity 0.886,test specificity 0.250\n",
      "第 4 折,train acc 0.808, test acc 0.796,test roc 0.577,test sensitivity 0.884,test specificity 0.364\n",
      "第 5 折,train acc 0.826, test acc 0.630,test roc 0.582,test sensitivity 0.903,test specificity 0.261\n",
      "第 6 折,train acc 0.802, test acc 0.778,test roc 0.616,test sensitivity 0.875,test specificity 0.357\n",
      "第 7 折,train acc 0.828, test acc 0.685,test roc 0.550,test sensitivity 0.865,test specificity 0.235\n",
      "第 8 折,train acc 0.818, test acc 0.759,test roc 0.634,test sensitivity 0.973,test specificity 0.353\n",
      "第 9 折,train acc 0.804, test acc 0.778,test roc 0.641,test sensitivity 0.925,test specificity 0.357\n",
      "第 10 折,train acc 0.820, test acc 0.741,test roc 0.593,test sensitivity 0.892,test specificity 0.294\n",
      "0.7293939393939394\n",
      "第 2 次交叉验证\n",
      "第 1 折,train acc 0.830, test acc 0.655,test roc 0.546,test sensitivity 0.853,test specificity 0.238\n",
      "第 2 折,train acc 0.818, test acc 0.782,test roc 0.532,test sensitivity 0.814,test specificity 0.250\n",
      "第 3 折,train acc 0.820, test acc 0.727,test roc 0.667,test sensitivity 0.897,test specificity 0.438\n",
      "第 4 折,train acc 0.828, test acc 0.759,test roc 0.590,test sensitivity 0.897,test specificity 0.333\n",
      "第 5 折,train acc 0.818, test acc 0.759,test roc 0.532,test sensitivity 0.829,test specificity 0.308\n",
      "第 6 折,train acc 0.816, test acc 0.722,test roc 0.550,test sensitivity 0.865,test specificity 0.235\n",
      "第 7 折,train acc 0.814, test acc 0.741,test roc 0.631,test sensitivity 0.941,test specificity 0.350\n",
      "第 8 折,train acc 0.816, test acc 0.704,test roc 0.629,test sensitivity 0.943,test specificity 0.316\n",
      "第 9 折,train acc 0.818, test acc 0.704,test roc 0.601,test sensitivity 0.914,test specificity 0.316\n",
      "第 10 折,train acc 0.824, test acc 0.741,test roc 0.622,test sensitivity 0.919,test specificity 0.353\n",
      "0.7293265993265994\n",
      "第 3 次交叉验证\n",
      "第 1 折,train acc 0.820, test acc 0.655,test roc 0.590,test sensitivity 0.917,test specificity 0.263\n",
      "第 2 折,train acc 0.814, test acc 0.655,test roc 0.590,test sensitivity 0.941,test specificity 0.286\n",
      "第 3 折,train acc 0.830, test acc 0.655,test roc 0.469,test sensitivity 0.833,test specificity 0.158\n",
      "第 4 折,train acc 0.820, test acc 0.722,test roc 0.595,test sensitivity 0.895,test specificity 0.375\n",
      "第 5 折,train acc 0.832, test acc 0.704,test roc 0.579,test sensitivity 0.865,test specificity 0.294\n",
      "第 6 折,train acc 0.806, test acc 0.833,test roc 0.655,test sensitivity 0.909,test specificity 0.400\n",
      "第 7 折,train acc 0.824, test acc 0.667,test roc 0.601,test sensitivity 0.886,test specificity 0.316\n",
      "第 8 折,train acc 0.818, test acc 0.667,test roc 0.606,test sensitivity 0.931,test specificity 0.280\n",
      "第 9 折,train acc 0.804, test acc 0.778,test roc 0.637,test sensitivity 0.881,test specificity 0.417\n",
      "第 10 折,train acc 0.816, test acc 0.796,test roc 0.623,test sensitivity 0.860,test specificity 0.455\n",
      "0.7130303030303031\n",
      "第 4 次交叉验证\n",
      "第 1 折,train acc 0.811, test acc 0.727,test roc 0.574,test sensitivity 0.897,test specificity 0.250\n",
      "第 2 折,train acc 0.814, test acc 0.745,test roc 0.545,test sensitivity 0.902,test specificity 0.286\n",
      "第 3 折,train acc 0.816, test acc 0.727,test roc 0.611,test sensitivity 0.895,test specificity 0.353\n",
      "第 4 折,train acc 0.804, test acc 0.667,test roc 0.500,test sensitivity 0.861,test specificity 0.167\n",
      "第 5 折,train acc 0.824, test acc 0.667,test roc 0.615,test sensitivity 0.914,test specificity 0.316\n",
      "第 6 折,train acc 0.818, test acc 0.685,test roc 0.514,test sensitivity 0.861,test specificity 0.222\n",
      "第 7 折,train acc 0.806, test acc 0.796,test roc 0.595,test sensitivity 0.854,test specificity 0.385\n",
      "第 8 折,train acc 0.798, test acc 0.759,test roc 0.639,test sensitivity 0.944,test specificity 0.389\n",
      "第 9 折,train acc 0.824, test acc 0.685,test roc 0.537,test sensitivity 0.865,test specificity 0.235\n",
      "第 10 折,train acc 0.824, test acc 0.667,test roc 0.529,test sensitivity 0.829,test specificity 0.316\n",
      "0.7125925925925926\n",
      "第 5 次交叉验证\n",
      "第 1 折,train acc 0.824, test acc 0.709,test roc 0.661,test sensitivity 0.941,test specificity 0.381\n",
      "第 2 折,train acc 0.824, test acc 0.673,test roc 0.616,test sensitivity 0.917,test specificity 0.316\n",
      "第 3 折,train acc 0.824, test acc 0.745,test roc 0.642,test sensitivity 0.927,test specificity 0.357\n",
      "第 4 折,train acc 0.826, test acc 0.778,test roc 0.603,test sensitivity 0.872,test specificity 0.400\n",
      "第 5 折,train acc 0.828, test acc 0.778,test roc 0.530,test sensitivity 0.857,test specificity 0.250\n",
      "第 6 折,train acc 0.832, test acc 0.667,test roc 0.562,test sensitivity 0.914,test specificity 0.211\n",
      "第 7 折,train acc 0.812, test acc 0.685,test roc 0.597,test sensitivity 0.889,test specificity 0.333\n",
      "第 8 折,train acc 0.812, test acc 0.704,test roc 0.559,test sensitivity 0.895,test specificity 0.250\n",
      "第 9 折,train acc 0.822, test acc 0.778,test roc 0.615,test sensitivity 0.897,test specificity 0.333\n",
      "第 10 折,train acc 0.828, test acc 0.685,test roc 0.606,test sensitivity 0.912,test specificity 0.300\n",
      "0.7201346801346801\n",
      "第 6 次交叉验证\n",
      "第 1 折,train acc 0.809, test acc 0.782,test roc 0.555,test sensitivity 0.860,test specificity 0.250\n",
      "第 2 折,train acc 0.807, test acc 0.745,test roc 0.612,test sensitivity 0.875,test specificity 0.400\n",
      "第 3 折,train acc 0.824, test acc 0.800,test roc 0.606,test sensitivity 0.905,test specificity 0.308\n",
      "第 4 折,train acc 0.824, test acc 0.741,test roc 0.559,test sensitivity 0.895,test specificity 0.250\n",
      "第 5 折,train acc 0.818, test acc 0.667,test roc 0.581,test sensitivity 0.912,test specificity 0.250\n",
      "第 6 折,train acc 0.826, test acc 0.741,test roc 0.569,test sensitivity 0.917,test specificity 0.278\n",
      "第 7 折,train acc 0.816, test acc 0.685,test roc 0.582,test sensitivity 0.892,test specificity 0.353\n",
      "第 8 折,train acc 0.834, test acc 0.685,test roc 0.523,test sensitivity 0.838,test specificity 0.235\n",
      "第 9 折,train acc 0.800, test acc 0.630,test roc 0.574,test sensitivity 0.909,test specificity 0.286\n",
      "第 10 折,train acc 0.822, test acc 0.630,test roc 0.537,test sensitivity 0.824,test specificity 0.250\n",
      "0.7105050505050505\n",
      "第 7 次交叉验证\n",
      "第 1 折,train acc 0.822, test acc 0.782,test roc 0.604,test sensitivity 0.900,test specificity 0.333\n",
      "第 2 折,train acc 0.830, test acc 0.727,test roc 0.529,test sensitivity 0.800,test specificity 0.333\n",
      "第 3 折,train acc 0.809, test acc 0.709,test roc 0.599,test sensitivity 0.865,test specificity 0.333\n",
      "第 4 折,train acc 0.808, test acc 0.704,test roc 0.505,test sensitivity 0.892,test specificity 0.176\n",
      "第 5 折,train acc 0.830, test acc 0.704,test roc 0.574,test sensitivity 0.914,test specificity 0.263\n",
      "第 6 折,train acc 0.802, test acc 0.759,test roc 0.577,test sensitivity 0.919,test specificity 0.235\n",
      "第 7 折,train acc 0.812, test acc 0.852,test roc 0.632,test sensitivity 0.864,test specificity 0.400\n",
      "第 8 折,train acc 0.824, test acc 0.630,test roc 0.562,test sensitivity 0.853,test specificity 0.300\n",
      "第 9 折,train acc 0.822, test acc 0.704,test roc 0.579,test sensitivity 0.892,test specificity 0.294\n",
      "第 10 折,train acc 0.826, test acc 0.630,test roc 0.543,test sensitivity 0.879,test specificity 0.238\n",
      "0.7199663299663299\n",
      "第 8 次交叉验证\n",
      "第 1 折,train acc 0.824, test acc 0.764,test roc 0.621,test sensitivity 0.947,test specificity 0.353\n",
      "第 2 折,train acc 0.805, test acc 0.691,test roc 0.686,test sensitivity 0.971,test specificity 0.400\n",
      "第 3 折,train acc 0.814, test acc 0.745,test roc 0.592,test sensitivity 0.900,test specificity 0.333\n",
      "第 4 折,train acc 0.822, test acc 0.741,test roc 0.623,test sensitivity 0.897,test specificity 0.400\n",
      "第 5 折,train acc 0.818, test acc 0.778,test roc 0.616,test sensitivity 0.875,test specificity 0.357\n",
      "第 6 折,train acc 0.830, test acc 0.722,test roc 0.574,test sensitivity 0.914,test specificity 0.263\n",
      "第 7 折,train acc 0.810, test acc 0.685,test roc 0.508,test sensitivity 0.857,test specificity 0.211\n",
      "第 8 折,train acc 0.802, test acc 0.778,test roc 0.611,test sensitivity 0.944,test specificity 0.333\n",
      "第 9 折,train acc 0.812, test acc 0.704,test roc 0.559,test sensitivity 0.868,test specificity 0.250\n",
      "第 10 折,train acc 0.812, test acc 0.722,test roc 0.559,test sensitivity 0.868,test specificity 0.250\n",
      "0.732962962962963\n",
      "第 9 次交叉验证\n",
      "第 1 折,train acc 0.820, test acc 0.727,test roc 0.510,test sensitivity 0.846,test specificity 0.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 2 折,train acc 0.809, test acc 0.745,test roc 0.636,test sensitivity 0.923,test specificity 0.375\n",
      "第 3 折,train acc 0.820, test acc 0.745,test roc 0.474,test sensitivity 0.829,test specificity 0.143\n",
      "第 4 折,train acc 0.818, test acc 0.759,test roc 0.648,test sensitivity 0.921,test specificity 0.375\n",
      "第 5 折,train acc 0.832, test acc 0.648,test roc 0.604,test sensitivity 0.968,test specificity 0.304\n",
      "第 6 折,train acc 0.830, test acc 0.722,test roc 0.577,test sensitivity 0.846,test specificity 0.333\n",
      "第 7 折,train acc 0.832, test acc 0.648,test roc 0.604,test sensitivity 0.935,test specificity 0.304\n",
      "第 8 折,train acc 0.838, test acc 0.722,test roc 0.559,test sensitivity 0.868,test specificity 0.250\n",
      "第 9 折,train acc 0.796, test acc 0.833,test roc 0.600,test sensitivity 0.837,test specificity 0.364\n",
      "第 10 折,train acc 0.826, test acc 0.667,test roc 0.548,test sensitivity 0.886,test specificity 0.211\n",
      "0.7218181818181818\n",
      "第 10 次交叉验证\n",
      "第 1 折,train acc 0.822, test acc 0.727,test roc 0.522,test sensitivity 0.846,test specificity 0.250\n",
      "第 2 折,train acc 0.818, test acc 0.727,test roc 0.578,test sensitivity 0.921,test specificity 0.235\n",
      "第 3 折,train acc 0.818, test acc 0.673,test roc 0.614,test sensitivity 0.939,test specificity 0.318\n",
      "第 4 折,train acc 0.802, test acc 0.759,test roc 0.530,test sensitivity 0.829,test specificity 0.231\n",
      "第 5 折,train acc 0.814, test acc 0.722,test roc 0.577,test sensitivity 0.872,test specificity 0.333\n",
      "第 6 折,train acc 0.818, test acc 0.852,test roc 0.593,test sensitivity 0.886,test specificity 0.300\n",
      "第 7 折,train acc 0.824, test acc 0.722,test roc 0.537,test sensitivity 0.865,test specificity 0.235\n",
      "第 8 折,train acc 0.822, test acc 0.667,test roc 0.514,test sensitivity 0.861,test specificity 0.167\n",
      "第 9 折,train acc 0.816, test acc 0.685,test roc 0.546,test sensitivity 0.886,test specificity 0.263\n",
      "第 10 折,train acc 0.816, test acc 0.685,test roc 0.628,test sensitivity 0.938,test specificity 0.318\n",
      "0.721986531986532\n",
      "平均测试集准确率 0.7211717171717175\n",
      "平均训练集准确率 0.8182116396795067\n",
      "roc 0.5826691001925461\n",
      "sensitivity 0.8908062544469412\n",
      "specificity 0.30032729864062147\n"
     ]
    }
   ],
   "source": [
    "# 使用交叉验证\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#x_train,x_test,y_train,y_test = train_test_split(df, label, test_size = 0.1)  \n",
    "\n",
    "\n",
    "train = df\n",
    "label == label\n",
    "\n",
    "accuracy = [] \n",
    "mean_1 = []\n",
    "mean_2 = []\n",
    "mean_3 = []\n",
    "train_accuracy = []\n",
    "k = 10\n",
    "for i in range(k):\n",
    "    kfold = KFold(n_splits=10,shuffle = True)\n",
    "    print('第 %d 次交叉验证' %(i+1))\n",
    "    test_acc_eve = []\n",
    "    cnt = 1\n",
    "    for train_index, test_index in kfold.split(train, label):\n",
    "\n",
    "        #print(test_index)\n",
    "        # train_index 就是分类的训练集的下标，test_index 就是分配的验证集的下标\n",
    "        this_train_x, this_train_y = train.iloc[train_index], label.iloc[train_index]  # 本组训练集\n",
    "        this_test_x, this_test_y = train.iloc[test_index], label.iloc[test_index]  # 本组验证集\n",
    "        # 训练本组的数据，并计算准确率\n",
    "        #model.fit(this_train_x, this_train_y)\n",
    "        \n",
    "        train_dataset = Data.TensorDataset(torch.tensor(this_train_x.values), torch.tensor(this_train_y.values))\n",
    "        test_dataset = Data.TensorDataset(torch.tensor(this_test_x.values), torch.tensor(this_test_y.values))\n",
    "        batch_size = 256\n",
    "        num_workers = 0\n",
    "        train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        for X, y in test_iter:\n",
    "            y_true = y\n",
    "        net = NET()\n",
    "        \n",
    "        num_epochs = 100\n",
    "        now_time = time.time()\n",
    "        losses = []\n",
    "        plot_roc = []\n",
    "        plot_sensitivity = []\n",
    "        plot_specificity = []\n",
    "        test_accuracies = []\n",
    "        train_accuracies = []\n",
    "        #损失函数和优化器\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        LR = 0.1\n",
    "        optimizer = torch.optim.SGD(\n",
    "                                    net.parameters(), \n",
    "                                    lr = LR,\n",
    "                                    weight_decay = 0.1\n",
    "                                     )\n",
    "        best_pred = []\n",
    "        best_y = []\n",
    "        best_test_acc = 0\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                for p in optimizer.param_groups:\n",
    "                    p['lr'] *= 0.95\n",
    "\n",
    "            train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for X, y in train_iter:\n",
    "                y_hat = net(X.to(torch.float32))\n",
    "                l = loss(y_hat,y).sum()#数据集损失函数的值=每个样本的损失函数值的和。            \n",
    "                optimizer.zero_grad()#对优化函数梯度清零\n",
    "                l.backward()#对损失函数求梯度\n",
    "                optimizer.step() \n",
    "\n",
    "                running_loss += l.item()\n",
    "                n += y.shape[0]\n",
    "            #print(train_acc_sum/n)\n",
    "            train_acc , train_pred=evaluate_accuracy(train_iter, net)\n",
    "            test_acc , test_pred= evaluate_accuracy(test_iter, net)\n",
    "            if test_acc>best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "            a,b,c = all_results(test_pred,y_true)\n",
    "            plot_roc .append(a)\n",
    "            plot_sensitivity .append(b)\n",
    "            plot_specificity .append(c)    \n",
    "                \n",
    "            test_accuracies.append(test_acc)\n",
    "            train_accuracies.append(train_acc)\n",
    "            losses.append(running_loss)\n",
    "            pre_time = now_time\n",
    "            now_time = time.time()\n",
    "        print('第 %d 折,train acc %.3f, test acc %.3f,test roc %.3f,test sensitivity %.3f,test specificity %.3f'\n",
    "          % (cnt,np.array(train_accuracies).max() ,np.array(test_accuracies).max(),np.array(plot_roc)[-10:].max(),np.array( plot_sensitivity)[-10:].max(),np.array(plot_specificity)[-10:].max()))\n",
    "        cnt = cnt+1\n",
    "        test_acc_eve.append(np.array(test_accuracies).max())\n",
    "        accuracy.append(np.array(test_accuracies).max())\n",
    "        train_accuracy.append(np.array(train_accuracies).max())\n",
    "        mean_1.append(np.array(plot_roc)[-10:].max())\n",
    "        mean_2.append(np.array(plot_sensitivity)[-10:].max())\n",
    "        mean_3.append(np.array(plot_specificity)[-10:].max())\n",
    "    print(np.array(test_acc_eve).mean())\n",
    "print('平均测试集准确率',np.array(accuracy).sum()/(k*10))\n",
    "print('平均训练集准确率',np.array(train_accuracy).sum()/(k*10))\n",
    "print('roc',np.array(mean_1).mean())\n",
    "print('sensitivity',np.array(mean_2).mean())\n",
    "print('specificity',np.array(mean_3).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
